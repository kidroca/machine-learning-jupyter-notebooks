{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Linear Model Tutorial\n",
    "\n",
    "In this tutorial, we will use the TF.Learn API in TensorFlow to solve a binary classification problem: Given census data about a person such as age, gender, education and occupation (the features), we will try to predict whether or not the person earns more than 50,000 dollars a year (the target label). We will train a logistic regression model, and given an individual's information our model will output a number between 0 and 1, which can be interpreted as the probability that the individual has an annual income of over 50,000 dollars.\n",
    "\n",
    "- Tutorial Url: https://www.tensorflow.org/tutorials/wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "           \"income_bracket\"]\n",
    "\n",
    "def read_csv(url, drop_first_row = False):\n",
    "    \n",
    "    skiprows = 1 if drop_first_row else 0\n",
    "    \n",
    "    csv = pd.read_csv(url, names = COLUMNS, skiprows = skiprows, skipinitialspace=True)\n",
    "    \n",
    "    return csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\")\n",
    "df_test = read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", drop_first_row = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'label'\n",
    "df_train[LABEL_COLUMN] = (df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "df_test[LABEL_COLUMN] = (df_test[\"income_bracket\"]).apply(lambda x: \">50K\" in x).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\", \"occupation\",\n",
    "                       \"relationship\", \"race\", \"gender\", \"native_country\"]\n",
    "CONTINUOUS_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "    \n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(df[k].size)],\n",
    "        values=df[k].values,\n",
    "        dense_shape=[df[k].size, 1])\n",
    "                          for k in CATEGORICAL_COLUMNS}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n",
    "    \n",
    "    # Converts the label column into a constant Tensor.\n",
    "    label = tf.constant(df[LABEL_COLUMN].values)\n",
    "    \n",
    "     # Returns the feature columns and the label.\n",
    "    return feature_cols, label\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(df_train)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Selecting and Engineering Features for the Model\n",
    "\n",
    "Selecting and crafting the right set of feature columns is key to learning an effective model. A feature column can be either one of the raw columns in the original dataframe (let's call them base feature columns), or any new columns created based on some transformations defined over one or multiple base columns (let's call them derived feature columns). Basically, \"feature column\" is an abstract concept of any raw or derived variable that can be used to predict the target label.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Categorical Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you know the set of all possible feature values of a column and there are only a few of them you can use\n",
    "# sparse_column_with_keys. Each keys get assigned an auto-increment ID starting from 0\n",
    "gender = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name = \"gender\", keys = [\"Female\", \"Male\"])\n",
    "\n",
    "# If we don't know the set of possible values in advance we can use `sparse_column_with_hash_bucket` instead\n",
    "# Each possible value will be hashed to an integer ID as they are encountered in training\n",
    "education = tf.contrib.layers.sparse_column_with_hash_bucket(\"education\", hash_bucket_size=1000)\n",
    "\n",
    "race = tf.contrib.layers.sparse_column_with_hash_bucket(\"race\", hash_bucket_size=100)\n",
    "marital_status = tf.contrib.layers.sparse_column_with_hash_bucket(\"marital_status\", hash_bucket_size=100)\n",
    "relationship = tf.contrib.layers.sparse_column_with_hash_bucket(\"relationship\", hash_bucket_size=100)\n",
    "workclass = tf.contrib.layers.sparse_column_with_hash_bucket(\"workclass\", hash_bucket_size=100)\n",
    "occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\"occupation\", hash_bucket_size=1000)\n",
    "native_country = tf.contrib.layers.sparse_column_with_hash_bucket(\"native_country\", hash_bucket_size=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Continuous Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "education_num = tf.contrib.layers.real_valued_column(\"education_num\")\n",
    "capital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\n",
    "capital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\n",
    "hours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Continuous Features Categorical through Bucketization\n",
    "\n",
    "Sometimes the relationship between a continuous feature and the label is not linear. As an hypothetical example, a person's income may grow with age in the early stage of one's career, then the growth may slow at some point, and finally the income decreases after retirement. \n",
    "\n",
    "Bucketization is a process of dividing the entire range of a continuous feature into a set of consecutive bins/buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_buckets = tf.contrib.layers.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "# boundaries is a list of bucket boundaries. In this case, there are 10 boundaries, \n",
    "# resulting in 11 age group buckets (from age 17 and below, 18-24, 25-29, ..., to 65 and over)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersecting Multiple Columns with CrossedColumn\n",
    "\n",
    "Using each base feature column separately may not be enough to explain the data. For example, the correlation between education and the label (earning > 50,000 dollars) may be different for different occupations. Therefore, if we only learn a single model weight for education=\"Bachelors\" and education=\"Masters\", we won't be able to capture every single education-occupation combination. To learn the differences between different feature combinations, we can add crossed feature columns to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "education_x_occupation = tf.contrib.layers.crossed_column([education, occupation], hash_bucket_size=int(1e4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a CrossedColumn over more than two columns. Each constituent column can be either a base feature column that is categorical (SparseColumn), a bucketized real-valued feature column (BucketizedColumn), or even another CrossColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_buckets_x_education_x_occupation = tf.contrib.layers.crossed_column(\n",
    "  [age_buckets, education, occupation], hash_bucket_size=int(1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining The Logistic Regression Model\n",
    "\n",
    "After processing the input data and defining all the feature columns, we're now ready to put them all together and build a Logistic Regression model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f465c00e390>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "feature_columns = [\n",
    "    gender, native_country, education, occupation, workclass, marital_status, race, \n",
    "    age_buckets, education_x_occupation, age_buckets_x_education_x_occupation\n",
    "]\n",
    "\n",
    "optimizer = tf.train.FtrlOptimizer(\n",
    "    learning_rate=0.1,\n",
    "    l1_regularization_strength=1.0,\n",
    "    l2_regularization_strength=1.0\n",
    ")\n",
    "\n",
    "m = tf.contrib.learn.LinearClassifier(feature_columns = feature_columns, optimizer = optimizer, model_dir = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column.py:1861: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column.py:1861: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpu4gVAG/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.69319, step = 1\n",
      "INFO:tensorflow:global_step/sec: 21.6791\n",
      "INFO:tensorflow:loss = 0.44801, step = 101 (4.615 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /tmp/tmpu4gVAG/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.408991.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearClassifier(params={'gradient_clip_norm': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._BinaryLogisticHead object at 0x7f465c00e210>, 'joint_weights': False, 'optimizer': <tensorflow.python.training.ftrl.FtrlOptimizer object at 0x7f465bfefed0>, 'feature_columns': [_SparseColumn(column_name='gender', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('Female', 'Male'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='native_country', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='workclass', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='marital_status', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='race', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string), _BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _CrossedColumn(columns=(_SparseColumn(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None), _CrossedColumn(columns=(_BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _SparseColumn(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string), _SparseColumn(column_name='occupation', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)), hash_bucket_size=1000000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None)]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(input_fn = train_input_fn, steps = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column.py:1861: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column.py:1861: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-05-28-22:48:41\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpu4gVAG/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-05-28-22:48:42\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.812419, accuracy/baseline_label_mean = 0.236226, accuracy/threshold_0.500000_mean = 0.812419, auc = 0.849563, global_step = 200, labels/actual_label_mean = 0.236226, labels/prediction_mean = 0.249005, loss = 0.404885, precision/positive_threshold_0.500000_mean = 0.707983, recall/positive_threshold_0.500000_mean = 0.350494\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "accuracy: 0.812419\n",
      "accuracy/baseline_label_mean: 0.236226\n",
      "accuracy/threshold_0.500000_mean: 0.812419\n",
      "auc: 0.849563\n",
      "global_step: 200\n",
      "labels/actual_label_mean: 0.236226\n",
      "labels/prediction_mean: 0.249005\n",
      "loss: 0.404885\n",
      "precision/positive_threshold_0.500000_mean: 0.707983\n",
      "recall/positive_threshold_0.500000_mean: 0.350494\n"
     ]
    }
   ],
   "source": [
    "results = m.evaluate(input_fn = eval_input_fn, steps = 1)\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
